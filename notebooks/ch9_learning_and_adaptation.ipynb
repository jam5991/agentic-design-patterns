{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chapter 9: Learning and Adaptation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Key Takeaways:\n",
                "- **Evolutionary Coding** uses LLMs to automatically discover and optimize algorithms through iterative code generation, evaluation, and selection.\n",
                "- **OpenEvolve** is an open-source implementation of evolutionary coding that can evolve entire code files across multiple programming languages.\n",
                "- **Multi-objective Optimization** allows simultaneous optimization of multiple metrics like correctness, performance, and code quality."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Heuristic: *Evolution over manual optimization.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "setup-cell",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Configuration Loaded:\n",
                        "   Project Root: /Users/jorgemartinez/Documents/projects/agentic-design-patterns\n",
                        "   Scripts Directory: /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import nest_asyncio\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Allow nested event loops (required for OpenEvolve in Jupyter)\n",
                "nest_asyncio.apply()\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "# --- Configuration ---\n",
                "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
                "SCRIPTS_DIR = os.path.join(PROJECT_ROOT, \"scripts\")\n",
                "\n",
                "print(f\"‚úÖ Configuration Loaded:\")\n",
                "print(f\"   Project Root: {PROJECT_ROOT}\")\n",
                "print(f\"   Scripts Directory: {SCRIPTS_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e623a25d",
            "metadata": {},
            "source": [
                "## OpenEvolve Overview\n",
                "\n",
                "OpenEvolve is an evolutionary coding agent that leverages Large Language Models (LLMs) to automatically optimize and discover algorithms. It implements the key concepts from Google DeepMind's AlphaEvolve system.\n",
                "\n",
                "### Key Components\n",
                "\n",
                "| Component | Description |\n",
                "|-----------|-------------|\n",
                "| **LLM Ensemble** | Uses multiple language models to generate diverse code modifications |\n",
                "| **Prompt Sampler** | Creates context-rich prompts incorporating past programs and scores |\n",
                "| **Evaluator Pool** | Tests generated programs and assigns scores based on defined metrics |\n",
                "| **Program Database** | Stores evolved programs using MAP-Elites algorithm for diversity |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d697b238",
            "metadata": {},
            "source": [
                "## The Evolution Pipeline\n",
                "\n",
                "```\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ  Initial Code   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   LLM Mutates   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Evaluate     ‚îÇ\n",
                "‚îÇ   (Program)     ‚îÇ     ‚îÇ   (Generate)    ‚îÇ     ‚îÇ   (Score)       ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "                                                         ‚îÇ\n",
                "                                                         ‚ñº\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ  Best Program   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ    Selection    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Program Database‚îÇ\n",
                "‚îÇ   (Output)      ‚îÇ     ‚îÇ   (Fitness)     ‚îÇ     ‚îÇ   (Archive)     ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "93620694",
            "metadata": {},
            "source": [
                "## Required Files\n",
                "\n",
                "OpenEvolve requires three key files to operate:\n",
                "\n",
                "1. **Initial Program** (`initial_program.py`) - The starting code to evolve\n",
                "2. **Evaluator** (`evaluator.py`) - Defines how to score evolved programs\n",
                "3. **Config** (`config.yaml`) - Evolution parameters and LLM settings"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "044d3ae7",
            "metadata": {},
            "source": [
                "### 1. Initial Program\n",
                "\n",
                "This is the starting point for evolution. Mark sections with `EVOLVE-BLOCK-START` and `EVOLVE-BLOCK-END` comments to indicate which parts can be modified."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "view-initial-program",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\"\"\"\n",
                        "Initial Program for OpenEvolve Evolution\n",
                        "=========================================\n",
                        "This is a simple sorting function that OpenEvolve will attempt to optimize.\n",
                        "The algorithm starts as a basic bubble sort and can be evolved into more\n",
                        "efficient implementations.\n",
                        "\n",
                        "This file contains the code that will be evolved by OpenEvolve.\n",
                        "Mark sections with # @evolve comments to indicate which parts can be modified.\n",
                        "\"\"\"\n",
                        "\n",
                        "\n",
                        "# EVOLVE-BLOCK-START\n",
                        "def sort_numbers(arr: list[int]) -> list[int]:\n",
                        "    \"\"\"\n",
                        "    Sort a list of numbers in ascending order.\n",
                        "    \n",
                        "    This is the target function for evolution. OpenEvolve will attempt\n",
                        "    to improve its performance while maintaining correctness.\n",
                        "    \n",
                        "    Args:\n",
                        "        arr: A list of integers to sort\n",
                        "        \n",
                        "    Returns:\n",
                        "        A new list with elements sorted in ascending order\n",
                        "    \"\"\"\n",
                        "    # Basic bubble sort - initial implementation to be evolved\n",
                        "    result = arr.copy()\n",
                        "    n = len(result)\n",
                        "    \n",
                        "    for i in range(n):\n",
                        "        for j in range(0, n - i - 1):\n",
                        "            if result[j] > result[j + 1]:\n",
                        "                result[j], result[j + 1] = result[j + 1], result[j]\n",
                        "    \n",
                        "    return result\n",
                        "# EVOLVE-BLOCK-END\n",
                        "\n",
                        "\n",
                        "if __name__ == \"__main__\":\n",
                        "    # Test the sorting function\n",
                        "    test_data = [64, 34, 25, 12, 22, 11, 90]\n",
                        "    sorted_data = sort_numbers(test_data)\n",
                        "    print(f\"Original: {test_data}\")\n",
                        "    print(f\"Sorted: {sorted_data}\")\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# View the initial program\n",
                "initial_program_path = os.path.join(SCRIPTS_DIR, \"initial_program.py\")\n",
                "\n",
                "with open(initial_program_path, 'r') as f:\n",
                "    print(f.read())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "03116ab3",
            "metadata": {},
            "source": [
                "### 2. Evaluator\n",
                "\n",
                "The evaluator defines the fitness function that scores each evolved program. It should return a dictionary of metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "view-evaluator",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\"\"\"\n",
                        "Evaluator for OpenEvolve Evolution\n",
                        "==================================\n",
                        "This module defines the evaluation function that scores evolved programs.\n",
                        "OpenEvolve uses this to determine which program variants are better.\n",
                        "\n",
                        "The evaluator returns a dictionary of metrics that OpenEvolve uses for\n",
                        "multi-objective optimization.\n",
                        "\n",
                        "IMPORTANT: OpenEvolve passes a FILE PATH to the evaluate function, not a module.\n",
                        "\"\"\"\n",
                        "\n",
                        "import time\n",
                        "import random\n",
                        "import importlib.util\n",
                        "import sys\n",
                        "from pathlib import Path\n",
                        "from typing import Any\n",
                        "\n",
                        "\n",
                        "def evaluate(program_path: str) -> dict[str, float]:\n",
                        "    \"\"\"\n",
                        "    Evaluate an evolved program and return performance metrics.\n",
                        "    \n",
                        "    This function is called by OpenEvolve to score each candidate program.\n",
                        "    It should return a dictionary of metric names to scores, where higher\n",
                        "    scores are better.\n",
                        "    \n",
                        "    Args:\n",
                        "        program_path: Path to the evolved program file\n",
                        "        \n",
                        "    Returns:\n",
                        "        Dictionary of metric names to scores (higher is better)\n",
                        "    \"\"\"\n",
                        "    metrics = {}\n",
                        "    \n",
                        "    # Load the program module from the file path\n",
                        "    program_module = _load_module(program_path)\n",
                        "    \n",
                        "    if program_module is None:\n",
                        "        # Failed to load module - return zero scores\n",
                        "        return {\n",
                        "            \"correctness\": 0.0,\n",
                        "            \"performance\": 0.0,\n",
                        "            \"combined_score\": 0.0\n",
                        "        }\n",
                        "    \n",
                        "    # Test correctness on various inputs\n",
                        "    correctness_score = _test_correctness(program_module)\n",
                        "    metrics[\"correctness\"] = correctness_score\n",
                        "    \n",
                        "    # Test performance (speed)\n",
                        "    performance_score = _test_performance(program_module)\n",
                        "    metrics[\"performance\"] = performance_score\n",
                        "    \n",
                        "    # Combined score for overall fitness (required by OpenEvolve)\n",
                        "    metrics[\"combined_score\"] = (correctness_score * 0.7) + (performance_score * 0.3)\n",
                        "    \n",
                        "    return metrics\n",
                        "\n",
                        "\n",
                        "def _load_module(program_path: str) -> Any:\n",
                        "    \"\"\"Load a Python module from a file path.\"\"\"\n",
                        "    try:\n",
                        "        spec = importlib.util.spec_from_file_location(\"evolved_program\", program_path)\n",
                        "        if spec is None or spec.loader is None:\n",
                        "            return None\n",
                        "        \n",
                        "        module = importlib.util.module_from_spec(spec)\n",
                        "        sys.modules[\"evolved_program\"] = module\n",
                        "        spec.loader.exec_module(module)\n",
                        "        return module\n",
                        "    except Exception as e:\n",
                        "        print(f\"Error loading module from {program_path}: {e}\")\n",
                        "        return None\n",
                        "\n",
                        "\n",
                        "def _test_correctness(program_module: Any) -> float:\n",
                        "    \"\"\"Test if the sorting function produces correct results.\"\"\"\n",
                        "    # Check if the module has sort_numbers function\n",
                        "    if not hasattr(program_module, \"sort_numbers\"):\n",
                        "        return 0.0\n",
                        "    \n",
                        "    test_cases = [\n",
                        "        [],\n",
                        "        [1],\n",
                        "        [2, 1],\n",
                        "        [3, 1, 2],\n",
                        "        [5, 4, 3, 2, 1],\n",
                        "        list(range(10, 0, -1)),\n",
                        "        [random.randint(0, 100) for _ in range(20)],\n",
                        "    ]\n",
                        "    \n",
                        "    passed = 0\n",
                        "    total = len(test_cases)\n",
                        "    \n",
                        "    for test_input in test_cases:\n",
                        "        try:\n",
                        "            result = program_module.sort_numbers(test_input.copy())\n",
                        "            expected = sorted(test_input)\n",
                        "            if result == expected:\n",
                        "                passed += 1\n",
                        "        except Exception:\n",
                        "            pass  # Failed test\n",
                        "    \n",
                        "    return passed / total if total > 0 else 0.0\n",
                        "\n",
                        "\n",
                        "def _test_performance(program_module: Any) -> float:\n",
                        "    \"\"\"Test the performance (speed) of the sorting function.\"\"\"\n",
                        "    # Check if the module has sort_numbers function\n",
                        "    if not hasattr(program_module, \"sort_numbers\"):\n",
                        "        return 0.0\n",
                        "    \n",
                        "    # Generate test data\n",
                        "    test_data = [random.randint(0, 10000) for _ in range(1000)]\n",
                        "    \n",
                        "    try:\n",
                        "        # Time the execution\n",
                        "        start_time = time.perf_counter()\n",
                        "        for _ in range(10):  # Run multiple times for accuracy\n",
                        "            program_module.sort_numbers(test_data.copy())\n",
                        "        elapsed_time = time.perf_counter() - start_time\n",
                        "        \n",
                        "        # Convert to a score (faster = higher score)\n",
                        "        # Baseline: 1 second = 0.5 score, faster is better\n",
                        "        performance_score = max(0.0, min(1.0, 1.0 - (elapsed_time / 2.0)))\n",
                        "        return performance_score\n",
                        "        \n",
                        "    except Exception:\n",
                        "        return 0.0\n",
                        "\n",
                        "\n",
                        "if __name__ == \"__main__\":\n",
                        "    # Test the evaluator with the initial program\n",
                        "    import os\n",
                        "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
                        "    initial_program_path = os.path.join(script_dir, \"initial_program.py\")\n",
                        "    \n",
                        "    results = evaluate(initial_program_path)\n",
                        "    print(\"Evaluation Results:\")\n",
                        "    for metric, score in results.items():\n",
                        "        print(f\"  {metric}: {score:.4f}\")\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# View the evaluator\n",
                "evaluator_path = os.path.join(SCRIPTS_DIR, \"evaluator.py\")\n",
                "\n",
                "with open(evaluator_path, 'r') as f:\n",
                "    print(f.read())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "828c537b",
            "metadata": {},
            "source": [
                "### 3. Configuration\n",
                "\n",
                "The config file controls evolution parameters, LLM settings, and evaluation options."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "view-config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "# OpenEvolve Configuration\n",
                        "# =========================\n",
                        "# This configuration file controls the evolution process.\n",
                        "\n",
                        "# ========================\n",
                        "# Top-Level Settings\n",
                        "# ========================\n",
                        "\n",
                        "# Number of iterations to run\n",
                        "max_iterations: 100\n",
                        "\n",
                        "# Save checkpoint every N iterations\n",
                        "checkpoint_interval: 10\n",
                        "\n",
                        "# Log level: DEBUG, INFO, WARNING, ERROR\n",
                        "log_level: \"INFO\"\n",
                        "\n",
                        "# Log directory (optional)\n",
                        "log_dir: \"../scripts/evolution_output/evolution_logs\"\n",
                        "\n",
                        "# Random seed for reproducibility\n",
                        "random_seed: 42\n",
                        "\n",
                        "# ========================\n",
                        "# LLM Configuration\n",
                        "# ========================\n",
                        "llm:\n",
                        "  # Primary model for code generation\n",
                        "  primary_model: \"gpt-4o-mini\"\n",
                        "  # Secondary model for diversity (optional)\n",
                        "  secondary_model: \"gpt-4o-mini\"\n",
                        "  # Temperature for generation (higher = more creative)\n",
                        "  temperature: 0.7\n",
                        "  # Maximum tokens per generation\n",
                        "  max_tokens: 2048\n",
                        "\n",
                        "# ========================\n",
                        "# Evaluator Configuration\n",
                        "# ========================\n",
                        "evaluator:\n",
                        "  # Disable cascade evaluation (our evaluator uses direct evaluation)\n",
                        "  cascade_evaluation: false\n",
                        "  # Number of parallel evaluations\n",
                        "  parallel_evaluations: 4\n",
                        "  # Timeout per evaluation (seconds)\n",
                        "  timeout: 30\n",
                        "\n",
                        "# ========================\n",
                        "# Database Configuration\n",
                        "# ========================\n",
                        "database:\n",
                        "  # Population size (number of programs to maintain)\n",
                        "  population_size: 10\n",
                        "  # Number of top programs to keep (elites)\n",
                        "  elite_ratio: 0.2\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# View the configuration\n",
                "config_path = os.path.join(SCRIPTS_DIR, \"config.yaml\")\n",
                "\n",
                "with open(config_path, 'r') as f:\n",
                "    print(f.read())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "584160e5",
            "metadata": {},
            "source": [
                "## Running Evolution\n",
                "\n",
                "OpenEvolve provides a simple `run_evolution` API that accepts file paths directly:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "openevolve-init",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÅ OpenEvolve Files:\n",
                        "   Initial Program: /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/initial_program.py\n",
                        "   Evaluator: /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/evaluator.py\n",
                        "   Config: /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/config.yaml\n",
                        "\n",
                        "‚úÖ Ready to run evolution!\n"
                    ]
                }
            ],
            "source": [
                "from openevolve import run_evolution\n",
                "\n",
                "# Define file paths\n",
                "initial_program_path = os.path.join(SCRIPTS_DIR, \"initial_program.py\")\n",
                "evaluator_path = os.path.join(SCRIPTS_DIR, \"evaluator.py\")\n",
                "config_path = os.path.join(SCRIPTS_DIR, \"config.yaml\")\n",
                "\n",
                "print(\"üìÅ OpenEvolve Files:\")\n",
                "print(f\"   Initial Program: {initial_program_path}\")\n",
                "print(f\"   Evaluator: {evaluator_path}\")\n",
                "print(f\"   Config: {config_path}\")\n",
                "print(\"\\n‚úÖ Ready to run evolution!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run-evolution",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-10 13:23:15,606 - INFO - Logging to ../scripts/evolution_output/evolution_logs/openevolve_20260110_132315.log\n",
                        "2026-01-10 13:23:15,624 - INFO - Set random seed to 42 for reproducibility\n",
                        "2026-01-10 13:23:15,658 - INFO - Initialized OpenAI LLM with model: gpt-4o-mini\n",
                        "2026-01-10 13:23:15,664 - INFO - Initialized LLM ensemble with models: gpt-4o-mini (weight: 0.83), gpt-4o-mini (weight: 0.17)\n",
                        "2026-01-10 13:23:15,675 - INFO - Initialized LLM ensemble with models: gpt-4o-mini (weight: 0.83), gpt-4o-mini (weight: 0.17)\n",
                        "2026-01-10 13:23:15,678 - INFO - Initialized prompt sampler\n",
                        "2026-01-10 13:23:15,679 - INFO - Set custom templates: system=evaluator_system_message, user=None\n",
                        "2026-01-10 13:23:15,679 - INFO - Initialized program database with 0 programs\n",
                        "2026-01-10 13:23:15,681 - INFO - Successfully loaded evaluation function from /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/evaluator.py\n",
                        "2026-01-10 13:23:15,681 - INFO - Initialized evaluator with /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/evaluator.py\n",
                        "2026-01-10 13:23:15,681 - INFO - Initialized OpenEvolve with /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/initial_program.py\n",
                        "2026-01-10 13:23:15,681 - INFO - Adding initial program to database\n",
                        "2026-01-10 13:23:15,877 - INFO - Evaluated program 4c421562-92f0-4d9a-839e-d0908bfa772a in 0.20s: correctness=1.0000, performance=0.9039, combined_score=0.9712\n",
                        "2026-01-10 13:23:15,877 - INFO - New MAP-Elites cell occupied in island 0: {'complexity': 5, 'diversity': 0}\n",
                        "2026-01-10 13:23:15,890 - INFO - Initialized process parallel controller with 4 workers\n",
                        "2026-01-10 13:23:15,891 - INFO - Set max None tasks per child\n",
                        "2026-01-10 13:23:15,893 - INFO - Started process pool with 4 processes\n",
                        "2026-01-10 13:23:15,893 - INFO - Using island-based evolution with 5 islands\n",
                        "2026-01-10 13:23:15,893 - INFO - Island Status:\n",
                        "2026-01-10 13:23:15,894 - INFO -  * Island 0: 1 programs, best=0.9712, avg=0.9712, diversity=0.00, gen=0 (best: 4c421562-92f0-4d9a-839e-d0908bfa772a)\n",
                        "2026-01-10 13:23:15,894 - INFO -    Island 1: 0 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0\n",
                        "2026-01-10 13:23:15,894 - INFO -    Island 2: 0 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0\n",
                        "2026-01-10 13:23:15,895 - INFO -    Island 3: 0 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0\n",
                        "2026-01-10 13:23:15,895 - INFO -    Island 4: 0 programs, best=0.0000, avg=0.0000, diversity=0.00, gen=0\n",
                        "2026-01-10 13:23:15,896 - INFO - Starting process-based evolution from iteration 1 for 2 iterations (total: 3)\n",
                        "2026-01-10 13:23:15,911 - INFO - Early stopping disabled\n",
                        "2026-01-10 13:23:24,820 - INFO - New MAP-Elites cell occupied in island 0: {'complexity': 0, 'diversity': 5}\n",
                        "2026-01-10 13:23:24,821 - INFO - New best program b7b950f2-6e30-41a3-98b3-2740b4c9bcd1 replaces 4c421562-92f0-4d9a-839e-d0908bfa772a (combined_score: 0.9712 ‚Üí 0.9999, +0.0287)\n",
                        "2026-01-10 13:23:24,822 - INFO - Iteration 1: Program b7b950f2-6e30-41a3-98b3-2740b4c9bcd1 (parent: 4c421562-92f0-4d9a-839e-d0908bfa772a) completed in 8.52s\n",
                        "2026-01-10 13:23:24,822 - INFO - Metrics: correctness=1.0000, performance=0.9996, combined_score=0.9999\n",
                        "2026-01-10 13:23:24,822 - INFO - üåü New best solution found at iteration 1: b7b950f2-6e30-41a3-98b3-2740b4c9bcd1\n",
                        "2026-01-10 13:23:27,939 - INFO - New MAP-Elites cell occupied in island 0: {'complexity': 0, 'diversity': 0}\n",
                        "2026-01-10 13:23:27,940 - INFO - New best program 4979d393-d8c8-49bd-b236-fbda7a98d07c replaces b7b950f2-6e30-41a3-98b3-2740b4c9bcd1 (combined_score: 0.9999 ‚Üí 0.9999, +0.0000)\n",
                        "2026-01-10 13:23:27,940 - INFO - Iteration 2: Program 4979d393-d8c8-49bd-b236-fbda7a98d07c (parent: 4c421562-92f0-4d9a-839e-d0908bfa772a) completed in 11.63s\n",
                        "2026-01-10 13:23:27,941 - INFO - Metrics: correctness=1.0000, performance=0.9996, combined_score=0.9999\n",
                        "2026-01-10 13:23:27,941 - INFO - üåü New best solution found at iteration 2: 4979d393-d8c8-49bd-b236-fbda7a98d07c\n",
                        "2026-01-10 13:23:27,942 - INFO - ‚úÖ Evolution completed - Maximum iterations reached\n",
                        "2026-01-10 13:23:28,067 - INFO - Stopped process pool\n",
                        "2026-01-10 13:23:28,067 - INFO - Using tracked best program: 4979d393-d8c8-49bd-b236-fbda7a98d07c\n",
                        "2026-01-10 13:23:28,067 - INFO - Evolution complete. Best program has metrics: correctness=1.0000, performance=0.9996, combined_score=0.9999\n",
                        "2026-01-10 13:23:28,070 - INFO - Saved best program to ../scripts/evolution_output/best/best_program.py with program info to ../scripts/evolution_output/best/best_program_info.json\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üèÜ Evolution Complete!\n",
                        "Best program metrics:\n",
                        "  correctness: 1.0000\n",
                        "  performance: 0.9996\n",
                        "  combined_score: 0.9999\n"
                    ]
                }
            ],
            "source": [
                "# Run the evolution process\n",
                "# Note: This requires an LLM API key (e.g., OPENAI_API_KEY or GOOGLE_API_KEY)\n",
                "# In a real scenario, you would run many more iterations\n",
                "\n",
                "result = run_evolution(\n",
                "    initial_program=initial_program_path,\n",
                "    evaluator=evaluator_path,\n",
                "    config=config_path,\n",
                "    iterations=2,  # Small number for demo; use 100+ in production\n",
                "    output_dir=\"../scripts/openevolve_output\"\n",
                ")\n",
                "\n",
                "print(f\"\\nüèÜ Evolution Complete!\")\n",
                "print(f\"Best program metrics:\")\n",
                "for name, value in result.metrics.items():\n",
                "    print(f\"  {name}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analyzing Results\n",
                "\n",
                "After evolution, you can examine the best program and its improvements:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "analyze-results",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìù Evolved Program Code:\n",
                        "==================================================\n",
                        "\"\"\"\n",
                        "Initial Program for OpenEvolve Evolution\n",
                        "=========================================\n",
                        "This is a simple sorting function that OpenEvolve will attempt to optimize.\n",
                        "The algorithm starts as a basic bubble sort and can be evolved into more\n",
                        "efficient implementations.\n",
                        "\n",
                        "This file contains the code that will be evolved by OpenEvolve.\n",
                        "Mark sections with # @evolve comments to indicate which parts can be modified.\n",
                        "\"\"\"\n",
                        "\n",
                        "\n",
                        "# EVOLVE-BLOCK-START\n",
                        "def sort_numbers(arr: list[int]) -> list[int]:\n",
                        "    \"\"\"\n",
                        "    Sort a list of numbers in ascending order.\n",
                        "    \n",
                        "    This is the target function for evolution. OpenEvolve will attempt\n",
                        "    to improve its performance while maintaining correctness.\n",
                        "    \n",
                        "    Args:\n",
                        "        arr: A list of integers to sort\n",
                        "        \n",
                        "    Returns:\n",
                        "        A new list with elements sorted in ascending order\n",
                        "    \"\"\"\n",
                        "    # Using Python's built-in sort for improved performance\n",
                        "    return sorted(arr)\n",
                        "# EVOLVE-BLOCK-END\n",
                        "\n",
                        "\n",
                        "if __name__ == \"__main__\":\n",
                        "    # Test the sorting function\n",
                        "    test_data = [64, 34, 25, 12, 22, 11, 90]\n",
                        "    sorted_data = sort_numbers(test_data)\n",
                        "    print(f\"Original: {test_data}\")\n",
                        "    print(f\"Sorted: {sorted_data}\")\n",
                        "\n",
                        "==================================================\n",
                        "\n",
                        "üìä Performance Improvement:\n",
                        "  Correctness: 100.00%\n",
                        "  Performance: 99.96%\n",
                        "  Overall Fitness: 0.00%\n"
                    ]
                }
            ],
            "source": [
                "# View the evolved code\n",
                "print(\"üìù Evolved Program Code:\")\n",
                "print(\"=\" * 50)\n",
                "print(result.best_code)\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Compare metrics with the original\n",
                "print(f\"\\nüìä Performance Improvement:\")\n",
                "print(f\"  Correctness: {result.metrics.get('correctness', 0):.2%}\")\n",
                "print(f\"  Performance: {result.metrics.get('performance', 0):.2%}\")\n",
                "print(f\"  Overall Fitness: {result.metrics.get('fitness', 0):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Alternative: Using the Low-Level API\n",
                "\n",
                "For more control, you can use the `OpenEvolve` class directly with a `Config` object:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "low-level-api",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-10 13:23:28,092 - INFO - Logging to ../scripts/evolution_output/evolution_logs/openevolve_20260110_132328.log\n",
                        "2026-01-10 13:23:28,092 - INFO - Logging to ../scripts/evolution_output/evolution_logs/openevolve_20260110_132328.log\n",
                        "2026-01-10 13:23:28,092 - INFO - Set random seed to 42 for reproducibility\n",
                        "2026-01-10 13:23:28,092 - INFO - Set random seed to 42 for reproducibility\n",
                        "2026-01-10 13:23:28,107 - INFO - Initialized LLM ensemble with models: gpt-4o-mini (weight: 0.83), gpt-4o-mini (weight: 0.17)\n",
                        "2026-01-10 13:23:28,107 - INFO - Initialized LLM ensemble with models: gpt-4o-mini (weight: 0.83), gpt-4o-mini (weight: 0.17)\n",
                        "2026-01-10 13:23:28,118 - INFO - Initialized LLM ensemble with models: gpt-4o-mini (weight: 0.83), gpt-4o-mini (weight: 0.17)\n",
                        "2026-01-10 13:23:28,118 - INFO - Initialized LLM ensemble with models: gpt-4o-mini (weight: 0.83), gpt-4o-mini (weight: 0.17)\n",
                        "2026-01-10 13:23:28,121 - INFO - Set custom templates: system=evaluator_system_message, user=None\n",
                        "2026-01-10 13:23:28,121 - INFO - Set custom templates: system=evaluator_system_message, user=None\n",
                        "2026-01-10 13:23:28,121 - INFO - Initialized program database with 0 programs\n",
                        "2026-01-10 13:23:28,121 - INFO - Initialized program database with 0 programs\n",
                        "2026-01-10 13:23:28,122 - INFO - Successfully loaded evaluation function from /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/evaluator.py\n",
                        "2026-01-10 13:23:28,122 - INFO - Successfully loaded evaluation function from /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/evaluator.py\n",
                        "2026-01-10 13:23:28,123 - INFO - Initialized evaluator with /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/evaluator.py\n",
                        "2026-01-10 13:23:28,123 - INFO - Initialized evaluator with /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/evaluator.py\n",
                        "2026-01-10 13:23:28,124 - INFO - Initialized OpenEvolve with /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/initial_program.py\n",
                        "2026-01-10 13:23:28,124 - INFO - Initialized OpenEvolve with /Users/jorgemartinez/Documents/projects/agentic-design-patterns/scripts/initial_program.py\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ OpenEvolve initialized with Config object!\n"
                    ]
                }
            ],
            "source": [
                "from openevolve import OpenEvolve\n",
                "from openevolve.config import Config\n",
                "\n",
                "# Load configuration from YAML file\n",
                "config = Config.from_yaml(os.path.join(SCRIPTS_DIR, \"config.yaml\"))\n",
                "\n",
                "# Initialize the OpenEvolve controller\n",
                "evolve = OpenEvolve(\n",
                "    initial_program_path=os.path.join(SCRIPTS_DIR, \"initial_program.py\"),\n",
                "    evaluation_file=os.path.join(SCRIPTS_DIR, \"evaluator.py\"),\n",
                "    config=config\n",
                ")\n",
                "\n",
                "print(\"‚úÖ OpenEvolve initialized with Config object!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Configuration Options\n",
                "\n",
                "| Parameter | Description | Typical Range |\n",
                "|-----------|-------------|---------------|\n",
                "| `iterations` | Number of evolution cycles | 100-10000 |\n",
                "| `population_size` | Programs maintained per generation | 10-100 |\n",
                "| `temperature` | LLM creativity (higher = more diverse) | 0.5-1.0 |\n",
                "| `mutation_rate` | Probability of code modification | 0.1-0.5 |\n",
                "| `elite_count` | Top programs preserved each generation | 1-5 |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Best Practices\n",
                "\n",
                "1. **Start Simple** - Begin with a basic, working implementation\n",
                "2. **Clear Metrics** - Define measurable, unambiguous evaluation criteria\n",
                "3. **Balanced Objectives** - Weight correctness higher than performance initially\n",
                "4. **Checkpointing** - Save progress regularly for long evolution runs\n",
                "5. **Diversity** - Use the MAP-Elites algorithm to avoid local optima"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "Learning and Adaptation through evolutionary coding represents a paradigm shift in algorithm development. Instead of manually optimizing code, you can:\n",
                "\n",
                "- **Define the goal** (evaluation metrics)\n",
                "- **Provide a starting point** (initial program)\n",
                "- **Let evolution discover improvements** (run OpenEvolve)\n",
                "\n",
                "This approach is particularly powerful for complex optimization problems where human intuition may miss novel solutions."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "asd",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
