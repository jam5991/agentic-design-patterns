{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-header",
            "metadata": {},
            "source": [
                "# Chapter 3: Parallelization"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "key-takeaways",
            "metadata": {},
            "source": [
                "Key Takeaways:\n",
                "- Parallelization allows agents to perform multiple independent tasks simultaneously.\n",
                "- This reduces overall latency compared to sequential execution.\n",
                "- It is useful for aggregating different perspectives or processing distinct aspects of a problem.\n",
                "- In LangChain, `RunnableParallel` (also known as the `Map` step) facilitates this pattern."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "heuristic",
            "metadata": {},
            "source": [
                "### Heuristic: *Many hands make light work.*"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-header",
            "metadata": {},
            "source": [
                "## Setup and Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "llm-init",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Configuration ---\n",
                "# Ensure your API key environment variable is set (e.g., GOOGLE_API_KEY)\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "try:\n",
                "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
                "    print(f\"Language model initialized: {llm.model}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error initializing language model: {e}\")\n",
                "    llm = None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "chains-header",
            "metadata": {},
            "source": [
                "## Define Independent Chains\n",
                "\n",
                "We define three distinct chains that can run independently:\n",
                "1. **Summary Chain**: Summarizes the topic.\n",
                "2. **Questions Chain**: Generates related questions.\n",
                "3. **Terms Chain**: Identifies key terms."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "define-chains",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Summary Chain\n",
                "summary_template = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"Summarize the following topic concisely:\"),\n",
                "    (\"user\", \"{topic}\")\n",
                "])\n",
                "if llm:\n",
                "    summarize_chain = summary_template | llm | StrOutputParser()\n",
                "else:\n",
                "    summarize_chain = None\n",
                "\n",
                "# 2. Questions Chain\n",
                "questions_template = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"Generate three interesting questions about the following topic:\"),\n",
                "    (\"user\", \"{topic}\")\n",
                "])\n",
                "if llm:\n",
                "    questions_chain = questions_template | llm | StrOutputParser()\n",
                "else:\n",
                "    questions_chain = None\n",
                "\n",
                "# 3. Terms Chain\n",
                "terms_template = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"Identify 5-10 key terms from the following topic, separated by commas:\"),\n",
                "    (\"user\", \"{topic}\")\n",
                "])\n",
                "if llm:\n",
                "    terms_chain = terms_template | llm | StrOutputParser()\n",
                "else:\n",
                "    terms_chain = None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "map-step-header",
            "metadata": {},
            "source": [
                "## Build the Parallel Chain (Map Step)\n",
                "\n",
                "The `RunnableParallel` component executes the defined chains simultaneously. It takes the input (topic) and passes it to each chain."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "map-chain",
            "metadata": {},
            "outputs": [],
            "source": [
                "if summarize_chain and questions_chain and terms_chain:\n",
                "    map_chain = RunnableParallel({\n",
                "        \"summary\": summarize_chain,\n",
                "        \"questions\": questions_chain,\n",
                "        \"key_terms\": terms_chain,\n",
                "        \"topic\": RunnablePassthrough()  # Pass the original topic through\n",
                "    })\n",
                "else:\n",
                "    map_chain = None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "synthesis-header",
            "metadata": {},
            "source": [
                "## Define Synthesis Step (Reduce Step)\n",
                "\n",
                "Combine the outputs from the parallel execution into a final coherent response."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "synthesis-chain",
            "metadata": {},
            "outputs": [],
            "source": [
                "synthesis_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"Based on the following information:\n",
                "\n",
                "Summary: {summary}\n",
                "\n",
                "Related Questions: {questions}\n",
                "\n",
                "Key Terms: {key_terms}\n",
                "\n",
                "Synthesize a comprehensive answer.\"\"\"),\n",
                "    (\"user\", \"Original topic: {topic}\")\n",
                "])\n",
                "\n",
                "if map_chain and llm:\n",
                "    full_parallel_chain = map_chain | synthesis_prompt | llm | StrOutputParser()\n",
                "else:\n",
                "    full_parallel_chain = None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "example-header",
            "metadata": {},
            "source": [
                "## Example Usage\n",
                "\n",
                "Run the full chain with a sample topic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "topic = \"The history of space exploration\"\n",
                "\n",
                "print(f\"--- Processing topic: '{topic}' ---\")\n",
                "if full_parallel_chain:\n",
                "    try:\n",
                "        result = full_parallel_chain.invoke(topic)\n",
                "        print(\"\\n--- Final Synthesized Response ---\")\n",
                "        print(result)\n",
                "    except Exception as e:\n",
                "        print(f\"Error during execution: {e}\")\n",
                "else:\n",
                "    print(\"Full chain not initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conclusion",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "This notebook demonstrated **Parallelization** using `RunnableParallel` in LangChain.\n",
                "\n",
                "Key benefits:\n",
                "- **Efficiency**: Running independent tasks concurrently reduces total execution time.\n",
                "- **Modularity**: Individual chains can be developed and tested separately.\n",
                "- **Rich Synthesis**: Aggregating diverse outputs leads to more comprehensive final results."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "asd",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
