{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chapter 11: Goal Setting and Monitoring"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Key Takeaways:\n",
                "- **Goal-Driven Iteration**: Define explicit goals and iterate until they are met.\n",
                "- **Feedback Loops**: Use an LLM to critique generated outputs and provide actionable feedback.\n",
                "- **LLM-as-Evaluator**: The same LLM can act as both generator and evaluator to determine goal completion.\n",
                "- **Iterative Refinement**: Each iteration builds upon the previous output, guided by feedback."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Heuristic: *Iterate until goals are met.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import random\n",
                "import re\n",
                "from pathlib import Path\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "# Add scripts directory to path\n",
                "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
                "SCRIPTS_DIR = os.path.join(PROJECT_ROOT, \"scripts\")\n",
                "sys.path.insert(0, SCRIPTS_DIR)\n",
                "\n",
                "from goal_agent_utils import clean_code_block, add_comment_header, to_snake_case\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Verify API key\n",
                "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
                "if not OPENAI_API_KEY:\n",
                "    raise EnvironmentError(\"‚ùå Please set the OPENAI_API_KEY environment variable.\")\n",
                "\n",
                "print(f\"‚úÖ Configuration Loaded:\")\n",
                "print(f\"   Project Root: {PROJECT_ROOT}\")\n",
                "print(f\"   Scripts Directory: {SCRIPTS_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "llm-init",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize OpenAI LLM\n",
                "print(\"üì° Initializing OpenAI LLM (gpt-4o)...\")\n",
                "\n",
                "llm = ChatOpenAI(\n",
                "    model=\"gpt-4o\",\n",
                "    temperature=0.3,\n",
                "    openai_api_key=OPENAI_API_KEY,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ LLM initialized successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "utility-section",
            "metadata": {},
            "source": [
                "## 1. Utility Functions\n",
                "\n",
                "These functions handle prompt generation, feedback evaluation, and goal verification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "generate-prompt",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_prompt(\n",
                "    use_case: str, goals: list[str], previous_code: str = \"\", feedback: str = \"\"\n",
                ") -> str:\n",
                "    \"\"\"Construct a prompt for code generation based on use case and goals.\"\"\"\n",
                "    print(\"üìù Constructing prompt for code generation...\")\n",
                "    \n",
                "    base_prompt = f\"\"\"\n",
                "You are an AI coding agent. Your job is to write Python code based on the following use case:\n",
                "\n",
                "Use Case: {use_case}\n",
                "\n",
                "Your goals are:\n",
                "{chr(10).join(f\"- {g.strip()}\" for g in goals)}\n",
                "\"\"\"\n",
                "    \n",
                "    if previous_code:\n",
                "        print(\"üîÑ Adding previous code to the prompt for refinement.\")\n",
                "        base_prompt += f\"\\nPreviously generated code:\\n{previous_code}\"\n",
                "    \n",
                "    if feedback:\n",
                "        print(\"üìã Including feedback for revision.\")\n",
                "        base_prompt += f\"\\nFeedback on previous version:\\n{feedback}\\n\"\n",
                "    \n",
                "    base_prompt += \"\\nPlease return only the revised Python code. Do not include comments or explanations outside the code.\"\n",
                "    \n",
                "    return base_prompt\n",
                "\n",
                "print(\"‚úÖ generate_prompt() defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feedback-funcs",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_code_feedback(code: str, goals: list[str]) -> str:\n",
                "    \"\"\"Use the LLM to evaluate code against defined goals.\"\"\"\n",
                "    print(\"üîç Evaluating code against the goals...\")\n",
                "    \n",
                "    feedback_prompt = f\"\"\"\n",
                "You are a Python code reviewer. A code snippet is shown below.\n",
                "\n",
                "Based on the following goals:\n",
                "{chr(10).join(f\"- {g.strip()}\" for g in goals)}\n",
                "\n",
                "Please critique this code and identify if the goals are met.\n",
                "Mention if improvements are needed for clarity, simplicity, correctness, edge case handling, or test coverage.\n",
                "\n",
                "Code:\n",
                "{code}\n",
                "\"\"\"\n",
                "    return llm.invoke(feedback_prompt)\n",
                "\n",
                "\n",
                "def goals_met(feedback_text: str, goals: list[str]) -> bool:\n",
                "    \"\"\"\n",
                "    Uses the LLM to evaluate whether the goals have been met based on the feedback text.\n",
                "    Returns True or False (parsed from LLM output).\n",
                "    \"\"\"\n",
                "    review_prompt = f\"\"\"\n",
                "You are an AI reviewer.\n",
                "\n",
                "Here are the goals:\n",
                "{chr(10).join(f\"- {g.strip()}\" for g in goals)}\n",
                "\n",
                "Here is the feedback on the code:\n",
                "\\\"\\\"\\\"\n",
                "{feedback_text}\n",
                "\\\"\\\"\\\"\n",
                "\n",
                "Based on the feedback above, have the goals been met?\n",
                "Respond with only one word: True or False.\n",
                "\"\"\"\n",
                "    response = llm.invoke(review_prompt).content.strip().lower()\n",
                "    return response == \"true\"\n",
                "\n",
                "print(\"‚úÖ get_code_feedback() and goals_met() defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save-code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_code_to_file(code: str, use_case: str) -> str:\n",
                "    \"\"\"Save the generated code to a file with a descriptive name.\"\"\"\n",
                "    print(\"üíæ Saving final code to file...\")\n",
                "    \n",
                "    summary_prompt = (\n",
                "        f\"Summarize the following use case into a single lowercase word or phrase, \"\n",
                "        f\"no more than 10 characters, suitable for a Python filename:\\n\\n{use_case}\"\n",
                "    )\n",
                "    raw_summary = llm.invoke(summary_prompt).content.strip()\n",
                "    short_name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", raw_summary.replace(\" \", \"_\").lower())[:10]\n",
                "    \n",
                "    random_suffix = str(random.randint(1000, 9999))\n",
                "    filename = f\"{short_name}_{random_suffix}.py\"\n",
                "    \n",
                "    # Save to scripts directory\n",
                "    filepath = Path(SCRIPTS_DIR) / \"generated\" / filename\n",
                "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    with open(filepath, \"w\") as f:\n",
                "        f.write(code)\n",
                "    \n",
                "    print(f\"‚úÖ Code saved to: {filepath}\")\n",
                "    return str(filepath)\n",
                "\n",
                "print(\"‚úÖ save_code_to_file() defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "main-agent-section",
            "metadata": {},
            "source": [
                "## 2. Main Agent Function\n",
                "\n",
                "The core iterative loop that generates code, gets feedback, and refines until goals are met."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run-agent",
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_code_agent(use_case: str, goals_input: str, max_iterations: int = 5) -> str:\n",
                "    \"\"\"\n",
                "    Run the goal-driven code generation agent.\n",
                "    \n",
                "    Args:\n",
                "        use_case: Description of what the code should accomplish\n",
                "        goals_input: Comma-separated list of goals\n",
                "        max_iterations: Maximum refinement iterations\n",
                "    \n",
                "    Returns:\n",
                "        Path to the saved code file\n",
                "    \"\"\"\n",
                "    goals = [g.strip() for g in goals_input.split(\",\")]\n",
                "    \n",
                "    print(f\"\\nüéØ Use Case: {use_case}\")\n",
                "    print(\"üéØ Goals:\")\n",
                "    for g in goals:\n",
                "        print(f\"   - {g}\")\n",
                "    \n",
                "    previous_code = \"\"\n",
                "    feedback = \"\"\n",
                "    \n",
                "    for i in range(max_iterations):\n",
                "        print(f\"\\n{'='*50}\")\n",
                "        print(f\"üîÅ Iteration {i + 1} of {max_iterations}\")\n",
                "        print(f\"{'='*50}\")\n",
                "        \n",
                "        # Generate code\n",
                "        prompt = generate_prompt(\n",
                "            use_case, goals, previous_code,\n",
                "            feedback if isinstance(feedback, str) else feedback.content\n",
                "        )\n",
                "        \n",
                "        print(\"üöß Generating code...\")\n",
                "        code_response = llm.invoke(prompt)\n",
                "        raw_code = code_response.content.strip()\n",
                "        code = clean_code_block(raw_code)\n",
                "        \n",
                "        print(\"\\nüßæ Generated Code:\")\n",
                "        print(\"-\" * 50)\n",
                "        print(code)\n",
                "        print(\"-\" * 50)\n",
                "        \n",
                "        # Get feedback\n",
                "        print(\"\\nüì§ Submitting code for feedback review...\")\n",
                "        feedback = get_code_feedback(code, goals)\n",
                "        feedback_text = feedback.content.strip()\n",
                "        \n",
                "        print(\"\\nüì• Feedback Received:\")\n",
                "        print(\"-\" * 50)\n",
                "        print(feedback_text)\n",
                "        print(\"-\" * 50)\n",
                "        \n",
                "        # Check if goals are met\n",
                "        if goals_met(feedback_text, goals):\n",
                "            print(\"\\n‚úÖ LLM confirms goals are met. Stopping iteration.\")\n",
                "            break\n",
                "        \n",
                "        print(\"\\n‚ö†Ô∏è Goals not fully met. Preparing for next iteration...\")\n",
                "        previous_code = code\n",
                "    \n",
                "    # Finalize and save\n",
                "    final_code = add_comment_header(code, use_case)\n",
                "    return save_code_to_file(final_code, use_case)\n",
                "\n",
                "print(\"‚úÖ run_code_agent() defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "example-section",
            "metadata": {},
            "source": [
                "## 3. Example Usage\n",
                "\n",
                "Let's generate code for finding the binary gap of a positive integer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "example-run",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nüß† Welcome to the AI Code Generation Agent\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# Define the use case and goals\n",
                "use_case = \"Write code to find BinaryGap of a given positive integer\"\n",
                "\n",
                "goals = (\n",
                "    \"Code simple to understand, \"\n",
                "    \"Functionally correct, \"\n",
                "    \"Handles comprehensive edge cases, \"\n",
                "    \"Takes positive integer input only, \"\n",
                "    \"Prints the results with few examples\"\n",
                ")\n",
                "\n",
                "# Run the agent\n",
                "result_path = run_code_agent(use_case, goals)\n",
                "\n",
                "print(f\"\\nüéâ Final code saved to: {result_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "view-result",
            "metadata": {},
            "source": [
                "### View Generated Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "display-code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the generated file\n",
                "with open(result_path, 'r') as f:\n",
                "    print(f.read())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "The Goal Setting and Monitoring pattern demonstrates how to use explicit goals and iterative feedback loops to guide an AI agent toward producing high-quality outputs. By leveraging the LLM as both generator and evaluator, the agent can self-correct and refine its work until the defined objectives are achieved."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "asd",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
