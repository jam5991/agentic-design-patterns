{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbc1ada",
   "metadata": {},
   "source": [
    "# Chapter 1: Prompt Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2570706",
   "metadata": {},
   "source": [
    "Key Takeaways: \n",
    "- Prompt Chaining breaks down complex tasks into a sequence of smaller, focused steps.\n",
    "- This is occasionally known as the Pipeline pattern.\n",
    "- Each step in a chain involves an LLM call or processing logic, using the output of the\n",
    "previous step as input.\n",
    "- This pattern improves the reliability and manageability of complex interactions with\n",
    "language models.\n",
    "- Frameworks like LangChain/LangGraph, and Google ADK provide robust tools to\n",
    "define, manage, and execute these multi-step sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef99271",
   "metadata": {},
   "source": [
    "High-level Design:\n",
    "\n",
    "1. Define prompt templates\n",
    "2. Bind prompts to models + parsers → Runnables\n",
    "3. Compose runnables into a workflow (LCEL graph)\n",
    "4. Invoke the composed runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47503ca7",
   "metadata": {},
   "source": [
    "### Heuristic: *required intermediates = chain*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Make sure your OPENAI_API_KEY is set in the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Language Model\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern1-header",
   "metadata": {},
   "source": [
    "## Pattern 1: Information Processing Workflows\n",
    "\n",
    "This pattern demonstrates processing raw information through multiple transformations. We'll simulate extracting content from a URL, summarizing it, extracting entities, searching a knowledge base, and generating a final report.\n",
    "\n",
    "**Use Case**: Document analysis and reporting pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Information Processing Workflows\n",
    "# 5-step chain: extract text -> summarize -> extract entities -> search KB -> generate report\n",
    "\n",
    "# Step 1: Extract text content (simulated)\n",
    "prompt_extract_text = ChatPromptTemplate.from_template(\n",
    "    \"Extract and clean the main text content from this document:\\n\\n{raw_content}\"\n",
    ")\n",
    "\n",
    "# Step 2: Summarize the cleaned text\n",
    "prompt_summarize = ChatPromptTemplate.from_template(\n",
    "    \"Provide a concise summary of the following text (2-3 sentences):\\n\\n{cleaned_text}\"\n",
    ")\n",
    "\n",
    "# Step 3: Extract entities\n",
    "prompt_extract_entities = ChatPromptTemplate.from_template(\n",
    "    \"Extract key entities (names, dates, locations, organizations) from this text in JSON format:\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "# Step 4: Search knowledge base (simulated with contextual query)\n",
    "prompt_search_kb = ChatPromptTemplate.from_template(\n",
    "    \"Based on these entities: {entities}, generate 2-3 relevant search queries for a knowledge base.\"\n",
    ")\n",
    "\n",
    "# Step 5: Generate final report\n",
    "prompt_generate_report = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Generate a comprehensive report incorporating:\n",
    "    \n",
    "Summary: {summary}\n",
    "Key Entities: {entities}\n",
    "Related Searches: {searches}\n",
    "\n",
    "Format the report in a professional manner with sections.\"\"\"\n",
    ")\n",
    "\n",
    "# Build the chain\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 1: Extract cleaned text\n",
    "extract_chain = prompt_extract_text | llm | parser\n",
    "\n",
    "# Step 2: Summarize the cleaned text\n",
    "summarize_chain = prompt_summarize | llm | parser\n",
    "\n",
    "# Step 3: Extract entities\n",
    "entities_chain = prompt_extract_entities | llm | parser\n",
    "\n",
    "# Step 4: Generate search queries\n",
    "search_chain = prompt_search_kb | llm | parser\n",
    "\n",
    "\n",
    "# Full information processing workflow using proper Runnables\n",
    "info_workflow = (\n",
    "    # First, extract cleaned text and pass through the raw_content\n",
    "    RunnablePassthrough.assign(\n",
    "        cleaned_text=extract_chain  # no need for lambda or reshaping since extract_chain expects {raw_content} or {\"raw_content\": sample_document which is what we pass in\n",
    "    )\n",
    "    # Then generate summary and entities based on cleaned_text\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: summarize_chain.invoke(\n",
    "            {\"cleaned_text\": x[\"cleaned_text\"]}\n",
    "        ),  # need reshaping since the current chain expects {cleaned_text} and has 2 keys\n",
    "        entities=lambda x: entities_chain.invoke(\n",
    "            {\"text\": x[\"cleaned_text\"]}\n",
    "        ),  # need reshaping since the current chain expects {cleaned_text} and has 3 keys\n",
    "    )\n",
    "    # Then generate searches based on entities\n",
    "    | RunnablePassthrough.assign(\n",
    "        searches=lambda x: search_chain.invoke({\"entities\": x[\"entities\"]})\n",
    "    )\n",
    "    # Finally generate the report using all the collected information\n",
    "    | prompt_generate_report\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "sample_document = \"\"\"Tesla Inc. announced on March 15, 2024, that CEO Elon Musk will unveil \n",
    "the company's new Gigafactory in Austin, Texas. The facility will produce 500,000 electric \n",
    "vehicles annually and employ over 10,000 workers. The announcement came during a press \n",
    "conference attended by Texas Governor Greg Abbott and Austin Mayor Kirk Watson.\"\"\"\n",
    "\n",
    "result = info_workflow.invoke({\"raw_content\": sample_document})\n",
    "print(\"\\n=== PATTERN 1: Information Processing Workflow ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern2-header",
   "metadata": {},
   "source": [
    "## Pattern 2: Complex Query Answering\n",
    "\n",
    "This pattern breaks down complex questions into sub-questions, researches each independently, and synthesizes a comprehensive answer.\n",
    "\n",
    "**Use Case**: Multi-faceted research questions requiring decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify core sub-questions\n",
    "prompt_identify_subqs = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Break down this complex question into 2-3 specific sub-questions:\n",
    "    \n",
    "Question: {query}\n",
    "\n",
    "Provide the sub-questions as a numbered list.\"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Research the causes\n",
    "prompt_research_causes = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Provide detailed information about the causes of the 1929 stock market crash.\n",
    "    Include economic, political, and social factors. (3-4 paragraphs)\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Research government response\n",
    "prompt_research_response = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Describe how the U.S. government responded to the 1929 stock market crash.\n",
    "    Include both immediate actions and longer-term policy changes. (3-4 paragraphs)\"\"\"\n",
    ")\n",
    "\n",
    "# Step 4: Synthesize the information\n",
    "prompt_synthesize = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Based on the following research, provide a comprehensive answer to the original question:\n",
    "\n",
    "Original Question: {original_query}\n",
    "\n",
    "Sub-questions identified:\n",
    "{subquestions}\n",
    "\n",
    "Research on Causes:\n",
    "{causes_info}\n",
    "\n",
    "Research on Government Response:\n",
    "{response_info}\n",
    "\n",
    "Synthesize this into a coherent, well-structured answer.\"\"\"\n",
    ")\n",
    "\n",
    "# Build the complex query chain using RunnablePassthrough\n",
    "subq_chain = prompt_identify_subqs | llm | parser\n",
    "causes_chain = prompt_research_causes | llm | parser\n",
    "response_chain = prompt_research_response | llm | parser\n",
    "\n",
    "complex_query_chain = (\n",
    "    # Step 1-4: Keep original query and identify sub-questions\n",
    "    RunnablePassthrough.assign(\n",
    "        original_query=lambda x: x[\n",
    "            \"query\"\n",
    "        ],  # no chain, cant just use x since x isnt define. x will be the state dict {\"query\": query}\n",
    "        subquestions=subq_chain,  # no need for lambda (or reshaping) since state dict is currently {\"query\": query}\n",
    "        causes_info=lambda x: causes_chain.invoke(\n",
    "            {}\n",
    "        ),  # causes_chain expects an empty dict\n",
    "        response_info=lambda x: response_chain.invoke(\n",
    "            {}\n",
    "        ),  # response_chain expects an empty dict\n",
    "    )\n",
    "    # Step 4: Synthesize everything into final answer\n",
    "    | prompt_synthesize\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Execute\n",
    "query = \"What were the main causes of the stock market crash in 1929, and how did government policy respond?\"\n",
    "result = complex_query_chain.invoke({\"query\": query})\n",
    "print(\"\\n=== PATTERN 2: Complex Query Answering ===\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "STATE 0\n",
    "x = {\"query\": query}\n",
    "complex_query_chain.invoke(x)\n",
    "\n",
    "STATE 1\n",
    "x = {\"original_query\": query, \"subquestions\": subq_chain.invoke(x)}\n",
    "\n",
    "STATE 2\n",
    "x = {\"original_query\": query, \"subquestions\": subq_chain.invoke(x), \"causes_info\": causes_chain.invoke({}), \"response_info\": response_chain.invoke({})}\n",
    "\n",
    "STATE 3\n",
    "x = {\"original_query\": query, \"subquestions\": subq_chain.invoke(x), \"causes_info\": causes_chain.invoke({}), \"response_info\": response_chain.invoke({})}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern3-header",
   "metadata": {},
   "source": [
    "## Pattern 3: Data Extraction and Transformation\n",
    "\n",
    "This pattern demonstrates iterative extraction and validation, with conditional re-extraction for missing or malformed data.\n",
    "\n",
    "**Use Case**: Converting unstructured documents to structured data with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 3: Data Extraction and Transformation\n",
    "# Iterative extraction with validation and retry logic\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Initial extraction attempt\n",
    "prompt_extract_fields = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Extract the following fields from this invoice document and return as JSON:\n",
    "- invoice_number\n",
    "- date\n",
    "- customer_name\n",
    "- customer_address\n",
    "- total_amount\n",
    "\n",
    "Invoice text:\n",
    "{invoice_text}\n",
    "\n",
    "Return only valid JSON, no additional text.\"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Validation and conditional re-extraction\n",
    "\n",
    "\n",
    "def validate_and_retry(invoice_text):\n",
    "    extraction_chain = prompt_extract_fields | llm | parser\n",
    "\n",
    "    # First attempt\n",
    "    result = extraction_chain.invoke({\"invoice_text\": invoice_text})\n",
    "    print(\"\\nFirst extraction attempt:\")\n",
    "    print(result)\n",
    "\n",
    "    try:\n",
    "        data = json.loads(result)\n",
    "        required_fields = [\n",
    "            \"invoice_number\",\n",
    "            \"date\",\n",
    "            \"customer_name\",\n",
    "            \"customer_address\",\n",
    "            \"total_amount\",\n",
    "        ]\n",
    "        missing = [f for f in required_fields if f not in data or not data[f]]\n",
    "\n",
    "        if missing:\n",
    "            # Retry with specific instructions for missing fields\n",
    "            print(f\"\\nMissing fields detected: {missing}\")\n",
    "            print(\"Attempting re-extraction...\")\n",
    "\n",
    "            retry_prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"The previous extraction was missing these fields: {missing_fields}\n",
    "                \n",
    "                Please carefully re-examine the invoice and extract these specific fields:\n",
    "                {missing_fields}\n",
    "\n",
    "                Previous extraction:\n",
    "                {previous_result}\n",
    "\n",
    "                Invoice text:\n",
    "                {invoice_text}\n",
    "\n",
    "                Return complete JSON with all fields.\"\"\"\n",
    "            )\n",
    "\n",
    "            retry_chain = retry_prompt | llm | parser\n",
    "            result = retry_chain.invoke(\n",
    "                {\n",
    "                    \"missing_fields\": \", \".join(missing),\n",
    "                    \"previous_result\": result,\n",
    "                    \"invoice_text\": invoice_text,\n",
    "                }\n",
    "            )\n",
    "            print(\"\\nRetry extraction result:\")\n",
    "            print(result)\n",
    "\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\nInvalid JSON format, requesting reformatting...\")\n",
    "        return result\n",
    "\n",
    "\n",
    "# Sample invoice\n",
    "invoice = \"\"\"ACME Corp Invoice\n",
    "Invoice #12345\n",
    "Date: 2024-01-15\n",
    "\n",
    "Bill To:\n",
    "John Smith\n",
    "123 Main Street\n",
    "New York, NY 10001\n",
    "\n",
    "Items:\n",
    "- Laptop Computer (Qty: 2) @ $1,200 = $2,400\n",
    "- Wireless Mouse (Qty: 5) @ $25 = $125\n",
    "\n",
    "Subtotal: $2,525\n",
    "Tax (8%): $202\n",
    "TOTAL: $2,727\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== PATTERN 3: Data Extraction and Transformation ===\")\n",
    "final_data = validate_and_retry(invoice)\n",
    "print(\"\\n=== Final Validated Data ===\")\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern4-header",
   "metadata": {},
   "source": [
    "## Pattern 4: Content Generation Workflows\n",
    "\n",
    "This pattern demonstrates progressive content creation: ideation → selection → outlining → drafting → refinement.\n",
    "\n",
    "**Use Case**: Structured content creation for articles, reports, or documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 4: Content Generation Workflows\n",
    "# 5-step chain: ideas -> selection -> outline -> drafting -> refinement\n",
    "\n",
    "# Step 1: Generate topic ideas\n",
    "prompt_generate_ideas = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Generate 5 engaging blog post topic ideas about: {interest}\n",
    "    \n",
    "Format: numbered list with brief description for each.\"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Auto-select best idea (or could be user selection)\n",
    "prompt_select_topic = ChatPromptTemplate.from_template(\n",
    "    \"\"\"From these topic ideas, select the most engaging and timely one:\n",
    "\n",
    "{ideas}\n",
    "\n",
    "Return only the selected topic title and a brief rationale.\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Create detailed outline\n",
    "prompt_create_outline = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Create a detailed outline for a blog post on this topic:\n",
    "\n",
    "{selected_topic}\n",
    "\n",
    "Include:\n",
    "- Introduction\n",
    "- 3-4 main points with sub-points\n",
    "- Conclusion\"\"\"\n",
    ")\n",
    "\n",
    "# Step 4: Draft sections\n",
    "prompt_draft_section = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Write a draft section for this part of the outline:\n",
    "\n",
    "Section: {section}\n",
    "\n",
    "Full outline for context:\n",
    "{full_outline}\n",
    "\n",
    "Previous sections:\n",
    "{previous_content}\n",
    "\n",
    "Write 2-3 paragraphs for this section.\"\"\"\n",
    ")\n",
    "\n",
    "# Step 5: Refine complete draft\n",
    "prompt_refine = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Review and refine this blog post draft for coherence, tone, and grammar:\n",
    "\n",
    "{complete_draft}\n",
    "\n",
    "Provide the refined version with improvements.\"\"\"\n",
    ")\n",
    "\n",
    "# Build content generation workflow\n",
    "ideas_chain = prompt_generate_ideas | llm | parser\n",
    "select_chain = prompt_select_topic | llm | parser\n",
    "outline_chain = prompt_create_outline | llm | parser\n",
    "section_chain = prompt_draft_section | llm | parser\n",
    "refine_chain = prompt_refine | llm | parser\n",
    "\n",
    "# Execute workflow (simplified version - full version would iterate through sections)\n",
    "\n",
    "\n",
    "def content_generation_workflow(interest):\n",
    "    # Generate and select topic\n",
    "    ideas = ideas_chain.invoke({\"interest\": interest})\n",
    "    print(\"\\n=== Generated Ideas ===\")\n",
    "    print(ideas)\n",
    "\n",
    "    selected = select_chain.invoke({\"ideas\": ideas})\n",
    "    print(\"\\n=== Selected Topic ===\")\n",
    "    print(selected)\n",
    "\n",
    "    outline = outline_chain.invoke({\"selected_topic\": selected})\n",
    "    print(\"\\n=== Outline ===\")\n",
    "    print(outline)\n",
    "\n",
    "    # Draft first section\n",
    "    intro = section_chain.invoke(\n",
    "        {\"section\": \"Introduction\", \"full_outline\": outline, \"previous_content\": \"\"}\n",
    "    )\n",
    "    print(\"\\n=== Introduction Draft ===\")\n",
    "    print(intro)\n",
    "\n",
    "    # For brevity, we'll refine just the intro\n",
    "    refined = refine_chain.invoke({\"complete_draft\": intro})\n",
    "    print(\"\\n=== Refined Introduction ===\")\n",
    "    print(refined)\n",
    "\n",
    "    return refined\n",
    "\n",
    "\n",
    "print(\"=== PATTERN 4: Content Generation Workflow ===\")\n",
    "result = content_generation_workflow(\"artificial intelligence in healthcare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern5-header",
   "metadata": {},
   "source": [
    "## Pattern 5: Conversational Agents with State\n",
    "\n",
    "This pattern maintains conversation context by building each turn's prompt with accumulated history.\n",
    "\n",
    "**Use Case**: Chatbots, virtual assistants, multi-turn dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 5: Conversational Agents with State\n",
    "# Maintain context across multiple conversation turns\n",
    "\n",
    "\n",
    "class ConversationalAgent:\n",
    "    def __init__(self):\n",
    "        self.conversation_state = {\"history\": [], \"user_info\": {}, \"intent_stack\": []}\n",
    "\n",
    "        # Prompt for intent and entity extraction\n",
    "        self.intent_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Analyze this user message and extract:\n",
    "            1. Primary intent (e.g., 'book_appointment', 'ask_question', 'provide_info')\n",
    "            2. Key entities (names, dates, locations, etc.)\n",
    "\n",
    "            User message: {user_message}\n",
    "\n",
    "            Conversation history: {history}\n",
    "\n",
    "            Return as JSON with 'intent' and 'entities' fields.\"\"\"\n",
    "        )\n",
    "\n",
    "        # Prompt for response generation\n",
    "        self.response_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Generate a helpful response based on:\n",
    "\n",
    "            User message: {user_message}\n",
    "            Detected intent: {intent_info}\n",
    "            Conversation history: {history}\n",
    "            User profile: {user_info}\n",
    "\n",
    "            Be conversational, helpful, and reference previous context when relevant.\"\"\"\n",
    "        )\n",
    "\n",
    "        self.intent_chain = self.intent_prompt | llm | parser\n",
    "        self.response_chain = self.response_prompt | llm | parser\n",
    "\n",
    "    def process_turn(self, user_message):\n",
    "        # Extract intent and entities\n",
    "        intent_info = self.intent_chain.invoke(\n",
    "            {\n",
    "                \"user_message\": user_message,\n",
    "                \"history\": str(self.conversation_state[\"history\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update state\n",
    "        self.conversation_state[\"history\"].append(\n",
    "            {\"user\": user_message, \"intent\": intent_info}\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        response = self.response_chain.invoke(\n",
    "            {\n",
    "                \"user_message\": user_message,\n",
    "                \"intent_info\": intent_info,\n",
    "                \"history\": str(\n",
    "                    self.conversation_state[\"history\"][:-1]\n",
    "                ),  # Exclude current\n",
    "                \"user_info\": str(self.conversation_state[\"user_info\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update history with response\n",
    "        self.conversation_state[\"history\"][-1][\"response\"] = response\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "# Demo conversation\n",
    "print(\"=== PATTERN 5: Conversational Agent with State ===\")\n",
    "agent = ConversationalAgent()\n",
    "\n",
    "turns = [\n",
    "    \"Hi, I'd like to book a doctor's appointment\",\n",
    "    \"I prefer next Tuesday afternoon\",\n",
    "    \"Yes, that works. My name is Sarah Johnson\",\n",
    "]\n",
    "\n",
    "for i, user_msg in enumerate(turns, 1):\n",
    "    print(f\"\\n--- Turn {i} ---\")\n",
    "    print(f\"User: {user_msg}\")\n",
    "    response = agent.process_turn(user_msg)\n",
    "    print(f\"Agent: {response}\")\n",
    "\n",
    "print(\"\\n=== Final Conversation State ===\")\n",
    "print(f\"Total turns: {len(agent.conversation_state['history'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern6-header",
   "metadata": {},
   "source": [
    "## Pattern 6: Code Generation and Refinement\n",
    "\n",
    "This pattern demonstrates iterative code development: requirements → pseudocode → implementation → analysis → refinement.\n",
    "\n",
    "**Use Case**: AI-assisted programming, code generation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 6: Code Generation and Refinement\n",
    "# 5-step chain: requirements -> pseudocode -> code -> analysis -> refinement\n",
    "\n",
    "# Step 1: Understand requirements and generate pseudocode\n",
    "prompt_pseudocode = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Based on this requirement, write detailed pseudocode:\n",
    "\n",
    "Requirement: {requirement}\n",
    "\n",
    "Break down the logic step-by-step in pseudocode format.\"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Generate initial code\n",
    "prompt_initial_code = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Convert this pseudocode into Python code:\n",
    "\n",
    "{pseudocode}\n",
    "\n",
    "Original requirement: {requirement}\n",
    "\n",
    "Provide clean, well-structured Python code with docstrings.\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Analyze for errors and improvements\n",
    "prompt_analyze_code = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Analyze this code for potential errors, edge cases, and improvements:\n",
    "\n",
    "{code}\n",
    "\n",
    "List specific issues found:\n",
    "1. Bugs or logical errors\n",
    "2. Missing edge case handling\n",
    "3. Performance issues\n",
    "4. Code style improvements\"\"\"\n",
    ")\n",
    "\n",
    "# Step 4: Refine the code\n",
    "prompt_refine_code = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Refine this code based on the identified issues:\n",
    "\n",
    "Original code:\n",
    "{code}\n",
    "\n",
    "Issues to address:\n",
    "{issues}\n",
    "\n",
    "Provide the improved version of the code.\"\"\"\n",
    ")\n",
    "\n",
    "# Step 5: Add documentation and tests\n",
    "prompt_add_docs = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Add comprehensive docstrings and 2-3 unit test cases for this code:\n",
    "\n",
    "{refined_code}\n",
    "\n",
    "Include:\n",
    "- Function/class docstrings\n",
    "- Pytest-style test cases\"\"\"\n",
    ")\n",
    "\n",
    "# Build code generation workflow\n",
    "pseudocode_chain = prompt_pseudocode | llm | parser\n",
    "code_chain = prompt_initial_code | llm | parser\n",
    "analyze_chain = prompt_analyze_code | llm | parser\n",
    "refine_chain = prompt_refine_code | llm | parser\n",
    "docs_chain = prompt_add_docs | llm | parser\n",
    "\n",
    "\n",
    "def code_generation_workflow(requirement):\n",
    "    # Generate pseudocode\n",
    "    pseudocode = pseudocode_chain.invoke({\"requirement\": requirement})\n",
    "    print(\"\\n=== Pseudocode ===\")\n",
    "    print(pseudocode)\n",
    "\n",
    "    # Generate initial code\n",
    "    code = code_chain.invoke({\"pseudocode\": pseudocode, \"requirement\": requirement})\n",
    "    print(\"\\n=== Initial Code ===\")\n",
    "    print(code)\n",
    "\n",
    "    # Analyze for issues\n",
    "    issues = analyze_chain.invoke({\"code\": code})\n",
    "    print(\"\\n=== Code Analysis ===\")\n",
    "    print(issues)\n",
    "\n",
    "    # Refine code\n",
    "    refined = refine_chain.invoke({\"code\": code, \"issues\": issues})\n",
    "    print(\"\\n=== Refined Code ===\")\n",
    "    print(refined)\n",
    "\n",
    "    # Add documentation\n",
    "    final = docs_chain.invoke({\"refined_code\": refined})\n",
    "    print(\"\\n=== Final Code with Docs and Tests ===\")\n",
    "    print(final)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "print(\"=== PATTERN 6: Code Generation and Refinement ===\")\n",
    "requirement = (\n",
    "    \"Create a function that finds the longest palindromic substring in a given string\"\n",
    ")\n",
    "result = code_generation_workflow(requirement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern7-header",
   "metadata": {},
   "source": [
    "## Pattern 7: Multimodal and Multi-step Reasoning\n",
    "\n",
    "This pattern demonstrates processing information from multiple modalities (image + text + structured data).\n",
    "\n",
    "**Use Case**: Invoice processing, document understanding, image-text analysis\n",
    "\n",
    "**Note**: This example uses GPT-4 Vision capabilities. Make sure you have access to vision-enabled models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern7-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 7: Multimodal and Multi-step Reasoning\n",
    "# 3-step chain: extract image text -> link labels -> interpret with table\n",
    "\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "# For vision capabilities, use GPT-4 Vision\n",
    "vision_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Load invoice image\n",
    "image_path = Path(\"../assets/invoice_sample.png\")\n",
    "\n",
    "# Step 1: Extract text from image\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"user\",\n",
    "                [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Extract all text from this invoice image. List each piece of information clearly.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | vision_llm | parser\n",
    "    return chain.invoke({})\n",
    "\n",
    "\n",
    "# Step 2: Link extracted text with labels\n",
    "prompt_link_labels = ChatPromptTemplate.from_template(\n",
    "    \"\"\"From the extracted invoice text, identify and label these key fields:\n",
    "\n",
    "Extracted text:\n",
    "{extracted_text}\n",
    "\n",
    "Create a labeled structure:\n",
    "- Invoice Number: \n",
    "- Date:\n",
    "- Customer:\n",
    "- Items:\n",
    "- Total Amount:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Interpret using business rules table\n",
    "prompt_interpret = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Using this labeled invoice data and business rules, determine the required action:\n",
    "\n",
    "Labeled Data:\n",
    "{labeled_data}\n",
    "\n",
    "Business Rules Table:\n",
    "| Total Amount | Customer Type | Action |\n",
    "|--------------|---------------|--------|\n",
    "| < $1000      | Any           | Auto-approve |\n",
    "| $1000-$5000  | Existing      | Manager review |\n",
    "| $1000-$5000  | New           | Director review |\n",
    "| > $5000      | Any           | Director approval required |\n",
    "\n",
    "Determine:\n",
    "1. Which rule applies\n",
    "2. Required action\n",
    "3. Any special notes or flags\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Build multimodal workflow\n",
    "link_chain = prompt_link_labels | llm | parser\n",
    "interpret_chain = prompt_interpret | llm | parser\n",
    "\n",
    "\n",
    "def multimodal_workflow(image_path):\n",
    "    # Step 1: Extract text from image\n",
    "    print(\"\\n=== Step 1: Extracting text from invoice image ===\")\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    print(extracted_text)\n",
    "\n",
    "    # Step 2: Link with labels\n",
    "    print(\"\\n=== Step 2: Linking extracted text with labels ===\")\n",
    "    labeled_data = link_chain.invoke({\"extracted_text\": extracted_text})\n",
    "    print(labeled_data)\n",
    "\n",
    "    # Step 3: Interpret with business rules\n",
    "    print(\"\\n=== Step 3: Interpreting with business rules ===\")\n",
    "    final_decision = interpret_chain.invoke({\"labeled_data\": labeled_data})\n",
    "    print(final_decision)\n",
    "\n",
    "    return final_decision\n",
    "\n",
    "\n",
    "print(\"=== PATTERN 7: Multimodal and Multi-step Reasoning ===\")\n",
    "if image_path.exists():\n",
    "    result = multimodal_workflow(image_path)\n",
    "else:\n",
    "    print(\n",
    "        f\"Image not found at {image_path}. Please ensure the invoice_sample.png exists in the assets folder.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated 7 comprehensive prompt chaining patterns:\n",
    "\n",
    "1. **Information Processing Workflows** - Multi-step document analysis pipeline\n",
    "2. **Complex Query Answering** - Breaking down and synthesizing research questions  \n",
    "3. **Data Extraction and Transformation** - Iterative extraction with validation\n",
    "4. **Content Generation Workflows** - Progressive content creation\n",
    "5. **Conversational Agents with State** - Context-aware multi-turn dialogues\n",
    "6. **Code Generation and Refinement** - Iterative code development\n",
    "7. **Multimodal and Multi-step Reasoning** - Processing images, text, and structured data\n",
    "\n",
    "Each pattern demonstrates how prompt chaining enables complex AI workflows by breaking tasks into manageable, sequential steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
